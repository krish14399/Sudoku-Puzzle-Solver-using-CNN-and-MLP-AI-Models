{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7338196,"sourceType":"datasetVersion","datasetId":4260331},{"sourceId":7409913,"sourceType":"datasetVersion","datasetId":4309805},{"sourceId":7409971,"sourceType":"datasetVersion","datasetId":4309850},{"sourceId":7416486,"sourceType":"datasetVersion","datasetId":4314436},{"sourceId":7438162,"sourceType":"datasetVersion","datasetId":4329106}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# Load your data\ntrain_images = np.load('/kaggle/input/sudokotask12/imgs-001.npy')  # Shape: (50000, 252, 252)\ntrain_labels = np.load('/kaggle/input/sudokotask12/known_values_labels.npy')  # Shape: (50000, 41, 3)\ntest_images = np.load('/kaggle/input/sudokotask12/imgs.npy')    # Shape: (10000, 252, 252)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T21:40:34.973412Z","iopub.execute_input":"2024-01-18T21:40:34.973704Z","iopub.status.idle":"2024-01-18T21:41:19.779118Z","shell.execute_reply.started":"2024-01-18T21:40:34.973678Z","shell.execute_reply":"2024-01-18T21:41:19.778127Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nimport numpy as np\n\n# def preprocess_images_batch(images, batch_size):\n#     num_images = images.shape[0]\n#     cell_size = 28\n#     for batch_start in range(0, num_images, batch_size):\n#         batch_end = min(batch_start + batch_size, num_images)\n#         processed_batch = np.zeros(((batch_end - batch_start) * 81, cell_size, cell_size))\n\n#         for idx in range(batch_start, batch_end):\n#             for row in range(9):\n#                 for col in range(9):\n#                     cell = images[idx, row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size]\n#                     processed_cell = cell / 255.0\n#                     processed_batch[(idx - batch_start) * 81 + row * 9 + col] = processed_cell\n\n#         # Here you can save each batch to disk or yield it if using a generator\n#         yield processed_batch\n\n# # Example usage\n# batch_size = 100  # Adjust this based on your system's memory capacity\n# for processed_batch in preprocess_images_batch(train_images, batch_size):\n#     # Save or process each batch\n#     pass\n\n# for processed_batch in preprocess_images_batch(train_images, batch_size):\n#     # Save or process each batch\n#     pass\n# def preprocess_images_simple(images):\n#     num_images = images.shape[0]\n#     cell_size = 28\n#     processed_images = np.zeros((num_images * 81, cell_size, cell_size))\n\n#     for idx in range(num_images):\n#         for row in range(9):\n#             for col in range(9):\n#                 # Extracting each cell\n#                 cell = images[idx, row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size]\n#                 # Normalize the cell\n#                 processed_cell = cell / 255.0\n#                 # Store in processed_images\n#                 processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n#     return processed_images\ndef preprocess_images_simple(images, new_size=(14, 14)):\n    num_images = images.shape[0]\n    processed_images = np.zeros((num_images * 81, new_size[0], new_size[1]))\n\n    for idx in range(num_images):\n        for row in range(9):\n            for col in range(9):\n                # Extract each cell\n                cell = images[idx, row*28:(row+1)*28, col*28:(col+1)*28]\n                # Resize the cell\n                resized_cell = cv2.resize(cell, new_size, interpolation=cv2.INTER_AREA)\n                # Normalize the cell\n                processed_cell = resized_cell / 255.0\n                # Store in processed_images\n                processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n    return processed_images\n\n# Process the entire datasets with reduced resolution\ntrain_images_processed = preprocess_images_simple(train_images)\ntest_images_processed = preprocess_images_simple(test_images)\n\n# # Process the entire datasets\n# train_images_processed = preprocess_images_simple(train_images)\n# test_images_processed = preprocess_images_simple(test_images)\n\n\n# train_images_processed = preprocess_images_batch(train_images, batch_size)\n# test_images_processed = preprocess_images_batch(test_images, batch_size)\n# def preprocess_images(images):\n#     num_images = images.shape[0]\n#     # Each image is divided into 9x9 cells, each cell is approximately 28x28\n#     cell_size = 28\n#     processed_images = np.zeros((num_images * 81, cell_size, cell_size))\n\n#     for idx in range(num_images):\n#         for row in range(9):\n#             for col in range(9):\n#                 # Extracting each cell\n#                 cell = images[idx, row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size]\n#                 # Normalize the cell\n#                 processed_cell = cell / 255.0\n#                 # Store in processed_images\n#                 processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n#     return processed_images\n\n# train_images_processed = preprocess_images(train_images)\n# test_images_processed = preprocess_images(test_images)","metadata":{"execution":{"iopub.status.busy":"2024-01-18T21:41:38.440369Z","iopub.execute_input":"2024-01-18T21:41:38.440750Z","iopub.status.idle":"2024-01-18T21:42:28.909795Z","shell.execute_reply.started":"2024-01-18T21:41:38.440718Z","shell.execute_reply":"2024-01-18T21:42:28.908976Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# We need to convert train_labels to a format suitable for training\ndef prepare_labels(labels):\n    # Flatten the labels into a 1D array\n    # Each image contributes 81 (9x9) cells\n    num_images = len(labels)\n    new_labels = np.zeros((num_images * 81), dtype=int)\n\n    for i, label in enumerate(labels):\n        for cell in label:\n            x, y, value = cell\n            x, y, value = int(x), int(y), int(value)  # Ensure integers\n            if 0 <= x < 9 and 0 <= y < 9:  # Check for bounds\n                # Convert 2D coordinates to 1D index and set the value\n                new_labels[i * 81 + x * 9 + y] = value\n\n    return new_labels\n\ntrain_labels_processed = prepare_labels(train_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T21:48:00.964304Z","iopub.execute_input":"2024-01-18T21:48:00.964695Z","iopub.status.idle":"2024-01-18T21:48:06.356876Z","shell.execute_reply.started":"2024-01-18T21:48:00.964662Z","shell.execute_reply":"2024-01-18T21:48:06.356094Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# def build_model():\n#     model = Sequential([\n#         Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n#         MaxPooling2D(2, 2),\n\n#         Conv2D(64, (3, 3), activation='relu', padding='same'),\n#         MaxPooling2D(2, 2),\n\n#         Conv2D(128, (3, 3), activation='relu', padding='same'),\n#         MaxPooling2D(2, 2),\n\n#         Flatten(),\n#         Dense(128, activation='relu'),\n#         Dropout(0.5),\n#         Dense(10, activation='softmax')\n#     ])\n#     return model\ndef build_model():\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(14, 14, 1), padding='same'),\n        MaxPooling2D(2, 2),\n\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(2, 2),\n\n        Flatten(),\n        Dense(64, activation='relu'),  # Reduced number of neurons\n        Dropout(0.5),\n        Dense(10, activation='softmax')  # 10 classes for digits 0-9\n    ])\n    return model\n\n# model = build_model()\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# model.summary()\n\nmodel = build_model()\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T21:48:10.183222Z","iopub.execute_input":"2024-01-18T21:48:10.183582Z","iopub.status.idle":"2024-01-18T21:48:23.296835Z","shell.execute_reply.started":"2024-01-18T21:48:10.183553Z","shell.execute_reply":"2024-01-18T21:48:23.295869Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 14, 14, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 7, 7, 32)          0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 3, 3, 64)          0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 576)               0         \n                                                                 \n dense (Dense)               (None, 64)                36928     \n                                                                 \n dropout (Dropout)           (None, 64)                0         \n                                                                 \n dense_1 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 56394 (220.29 KB)\nTrainable params: 56394 (220.29 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming train_images_processed and train_labels_processed are your full datasets\n# Split the first 25,000 for training and the rest for potential validation and testing\ntrain_images_subset = train_images_processed[:50000]\ntrain_labels_subset = train_labels_processed[:50000]\n\n# Further split the training data to create a validation set\n# Here, I'll use 20% of the 25,000 images for validation, but you can adjust this percentage\ntrain_images_final, val_images, train_labels_final, val_labels = train_test_split(\n    train_images_subset, train_labels_subset, test_size=0.2, random_state=42)\n\n# Reshape the datasets for the CNN\ntrain_images_final_reshaped = np.expand_dims(train_images_final, axis=-1)\nval_images_reshaped = np.expand_dims(val_images, axis=-1)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T21:48:29.695999Z","iopub.execute_input":"2024-01-18T21:48:29.696964Z","iopub.status.idle":"2024-01-18T21:48:30.092907Z","shell.execute_reply.started":"2024-01-18T21:48:29.696920Z","shell.execute_reply":"2024-01-18T21:48:30.091894Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(train_images_final_reshaped, train_labels_final, \n          validation_data=(val_images_reshaped, val_labels),\n          epochs=10, batch_size=8)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T21:48:40.800072Z","iopub.execute_input":"2024-01-18T21:48:40.800484Z","iopub.status.idle":"2024-01-18T21:51:37.046846Z","shell.execute_reply.started":"2024-01-18T21:48:40.800444Z","shell.execute_reply":"2024-01-18T21:51:37.045906Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/10\n5000/5000 [==============================] - 21s 3ms/step - loss: 0.2653 - accuracy: 0.9139 - val_loss: 0.0599 - val_accuracy: 0.9809\nEpoch 2/10\n5000/5000 [==============================] - 18s 4ms/step - loss: 0.0963 - accuracy: 0.9719 - val_loss: 0.0369 - val_accuracy: 0.9887\nEpoch 3/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0665 - accuracy: 0.9804 - val_loss: 0.0390 - val_accuracy: 0.9882\nEpoch 4/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0507 - accuracy: 0.9844 - val_loss: 0.0312 - val_accuracy: 0.9915\nEpoch 5/10\n5000/5000 [==============================] - 18s 4ms/step - loss: 0.0423 - accuracy: 0.9866 - val_loss: 0.0336 - val_accuracy: 0.9908\nEpoch 6/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0380 - accuracy: 0.9884 - val_loss: 0.0223 - val_accuracy: 0.9934\nEpoch 7/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0336 - accuracy: 0.9894 - val_loss: 0.0282 - val_accuracy: 0.9926\nEpoch 8/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0300 - accuracy: 0.9906 - val_loss: 0.0305 - val_accuracy: 0.9912\nEpoch 9/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0258 - accuracy: 0.9919 - val_loss: 0.0325 - val_accuracy: 0.9915\nEpoch 10/10\n5000/5000 [==============================] - 17s 3ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.0309 - val_accuracy: 0.9931\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7a80c0f389a0>"},"metadata":{}}]},{"cell_type":"code","source":"model.save('/kaggle/working/model')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T22:05:33.551142Z","iopub.execute_input":"2024-01-18T22:05:33.552030Z","iopub.status.idle":"2024-01-18T22:05:34.631043Z","shell.execute_reply.started":"2024-01-18T22:05:33.551998Z","shell.execute_reply":"2024-01-18T22:05:34.630032Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport shutil\n\n# Assume 'your_model_path' is the directory where your model is saved\nmodel_path = '/kaggle/working/model'\n\n# Save the model\nmodel = tf.keras.models.load_model(model_path)\n\n# Specify the path for the zip file\nzip_file_path = 'task1model.zip'\n\n# Use shutil to compress the model directory into a zip file\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T22:05:38.211743Z","iopub.execute_input":"2024-01-18T22:05:38.212124Z","iopub.status.idle":"2024-01-18T22:05:38.668514Z","shell.execute_reply.started":"2024-01-18T22:05:38.212092Z","shell.execute_reply":"2024-01-18T22:05:38.667479Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/task1model.zip'"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\n# Load the model from the file\nloaded_model = tf.keras.models.load_model('/kaggle/input/task1zipfile')","metadata":{"execution":{"iopub.status.busy":"2024-01-19T23:48:10.535769Z","iopub.execute_input":"2024-01-19T23:48:10.536311Z","iopub.status.idle":"2024-01-19T23:48:23.412591Z","shell.execute_reply.started":"2024-01-19T23:48:10.536284Z","shell.execute_reply":"2024-01-19T23:48:23.411570Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\n\nimport numpy as np\ndef preprocess_images_simple(images, new_size=(14, 14)):\n    num_images = images.shape[0]\n    processed_images = np.zeros((num_images * 81, new_size[0], new_size[1]))\n\n    for idx in range(num_images):\n        for row in range(9):\n            for col in range(9):\n                # Extract each cell\n                cell = images[idx, row*28:(row+1)*28, col*28:(col+1)*28]\n                # Resize the cell\n                resized_cell = cv2.resize(cell, new_size, interpolation=cv2.INTER_AREA)\n                # Normalize the cell\n                processed_cell = resized_cell / 255.0\n                # Store in processed_images\n                processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n    return processed_images","metadata":{"execution":{"iopub.status.busy":"2024-01-19T23:48:23.414290Z","iopub.execute_input":"2024-01-19T23:48:23.414580Z","iopub.status.idle":"2024-01-19T23:48:23.596828Z","shell.execute_reply.started":"2024-01-19T23:48:23.414555Z","shell.execute_reply":"2024-01-19T23:48:23.596058Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_images = np.load('/kaggle/input/sudokotask2-dataset/imgs-002.npy')    # Shape: (10000, 252, 252)\ntest_images_processed = preprocess_images_simple(test_images)\ntest_images_reshaped = np.expand_dims(test_images_processed, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T23:50:00.762047Z","iopub.execute_input":"2024-01-19T23:50:00.762969Z","iopub.status.idle":"2024-01-19T23:51:05.977819Z","shell.execute_reply.started":"2024-01-19T23:50:00.762934Z","shell.execute_reply":"2024-01-19T23:51:05.976698Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Predict\npredictions = loaded_model.predict(test_images_reshaped,batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T23:51:05.979368Z","iopub.execute_input":"2024-01-19T23:51:05.979700Z","iopub.status.idle":"2024-01-20T00:07:02.733937Z","shell.execute_reply.started":"2024-01-19T23:51:05.979674Z","shell.execute_reply":"2024-01-20T00:07:02.733007Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"405000/405000 [==============================] - 632s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_submission_file(predictions, num_images):\n    with open('submission6.csv', 'w') as f:\n        f.write('id,value\\n')\n        for img_id in range(num_images):\n            for position in range(81):  # Iterate over each cell in the grid\n                x = position // 9  # x-coordinate (row)\n                y = position % 9   # y-coordinate (column)\n                cell_id = f\"{img_id}_{x}{y}\"\n                # Calculate the index in the predictions array\n                pred_index = img_id * 81 + position\n                # Get the most likely digit for this cell\n                value = np.argmax(predictions[pred_index])\n                f.write(f\"{cell_id},{value}\\n\")\n\ncreate_submission_file(predictions, num_images=10000)  # Replace 50000 with your actual number of images","metadata":{"execution":{"iopub.status.busy":"2024-01-18T22:29:43.527003Z","iopub.execute_input":"2024-01-18T22:29:43.527809Z","iopub.status.idle":"2024-01-18T22:29:48.024052Z","shell.execute_reply.started":"2024-01-18T22:29:43.527773Z","shell.execute_reply":"2024-01-18T22:29:48.023120Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def create_submission_file(predictions, output_file='submission.csv'):\n    with open(output_file, 'w') as f:\n        f.write('id,value\\n')\n        \n        for img_id, prediction in enumerate(predictions):\n            row_values = [str(int(round(value))) for value in prediction]\n            row_id = f\"{img_id}\"\n            f.write(f\"{row_id},\" + ''.join(row_values) + '\\n')\n    \ncreate_submission_file(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T00:07:02.770160Z","iopub.execute_input":"2024-01-20T00:07:02.770482Z","iopub.status.idle":"2024-01-20T00:08:48.275221Z","shell.execute_reply.started":"2024-01-20T00:07:02.770454Z","shell.execute_reply":"2024-01-20T00:08:48.274405Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Replace 'input_file.csv' and 'output_file.csv' with your actual file names\ninput_file = '/kaggle/input/outputfile/Train Image submission.csv'\noutput_file = '/kaggle/working/Submission1.csv'\n\n# Read the CSV file\nwith open(input_file, 'r') as infile:\n    reader = csv.reader(infile)\n    header = next(reader)  # Read and skip the header\n\n    # Open the output CSV file for writing\n    with open(output_file, 'w', newline='') as outfile:\n        writer = csv.writer(outfile)\n\n        # Write the modified header to the output file\n        writer.writerow(['QUIZZ'])\n\n        # Process each row in the input file\n        for row in reader:\n            # Ensure the row has enough columns\n            if len(row) >= 2:\n                # Concatenate the 'value' column\n                quizz_values = row[1]  # Assuming 'value' is at index 1\n                # Write the concatenated value to the output file\n                writer.writerow([quizz_values])\n            else:\n                print(f\"Skipping row {reader.line_num} due to insufficient columns.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T01:04:09.151941Z","iopub.execute_input":"2024-01-20T01:04:09.152314Z","iopub.status.idle":"2024-01-20T01:04:14.267137Z","shell.execute_reply.started":"2024-01-20T01:04:09.152285Z","shell.execute_reply":"2024-01-20T01:04:14.266179Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Replace 'input_file.csv' and 'output_file.csv' with your actual file names\ninput_file = '/kaggle/input/outputfile/Train Image submission.csv'\noutput_file = '/kaggle/working/Submission2.csv'\n\n# Read the CSV file\nwith open(input_file, 'r') as infile:\n    reader = csv.reader(infile)\n    header = next(reader)  # Read and skip the header\n\n    # Open the output CSV file for writing\n    with open(output_file, 'w', newline='') as outfile:\n        writer = csv.writer(outfile)\n\n        # Write the modified header to the output file\n        writer.writerow(['QUIZZ'])\n\n        # Combine all 'value' values into a single line\n        quizz_values = ''.join(row[1] for row in reader)\n\n        # Write the concatenated values to the output file\n        writer.writerow([quizz_values])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T01:10:08.819877Z","iopub.execute_input":"2024-01-20T01:10:08.820372Z","iopub.status.idle":"2024-01-20T01:10:11.094331Z","shell.execute_reply.started":"2024-01-20T01:10:08.820342Z","shell.execute_reply":"2024-01-20T01:10:11.093094Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Replace 'input_file.csv' and 'output_file.csv' with your actual file names\ninput_file = '/kaggle/input/outputfile/Train Image submission.csv'\noutput_file = '/kaggle/working/Submission3.csv'\n\n# Read the CSV file\nwith open(input_file, 'r') as infile:\n    reader = csv.reader(infile)\n    header = next(reader)  # Read and skip the header\n\n    # Open the output CSV file for writing\n    with open(output_file, 'w', newline='') as outfile:\n        writer = csv.writer(outfile)\n\n        # Write the modified header to the output file\n        writer.writerow(['QUIZZ'])\n\n        # Process each row in the input file\n        for row in reader:\n            # Ensure the row has enough columns\n            if len(row) >= 2:\n                # Combine the first 81 'value' values into a single line\n                quizz_values = ''.join(row[1:82])  # Assuming 'value' starts at index 1\n                # Write the concatenated values to the output file\n                writer.writerow([quizz_values])\n            else:\n                print(f\"Skipping row {reader.line_num} due to insufficient columns.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T01:19:09.806797Z","iopub.execute_input":"2024-01-20T01:19:09.807217Z","iopub.status.idle":"2024-01-20T01:19:09.863347Z","shell.execute_reply.started":"2024-01-20T01:19:09.807182Z","shell.execute_reply":"2024-01-20T01:19:09.862187Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Ensure the row has enough columns\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# Combine the first 81 'value' values into a single line\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m         quizz_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mcolumn\u001b[49m[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m82\u001b[39m])  \u001b[38;5;66;03m# Assuming 'value' starts at index 1\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Write the concatenated values to the output file\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow([quizz_values])\n","\u001b[0;31mNameError\u001b[0m: name 'column' is not defined"],"ename":"NameError","evalue":"name 'column' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import csv\n\n# Replace 'input_file.csv' and 'output_file.csv' with your actual file names\ninput_file = '/kaggle/input/outputfile/Train Image submission.csv'\noutput_file = '/kaggle/working/Submission4.csv'\n\n# Read the CSV file\nwith open(input_file, 'r') as infile:\n    reader = csv.reader(infile)\n    header = next(reader)  # Read and skip the header\n\n    # Open the output CSV file for writing\n    with open(output_file, 'w', newline='') as outfile:\n        writer = csv.writer(outfile)\n\n        # Write the modified header to the output file\n        writer.writerow(['QUIZZ'])\n\n        # Process each row in the input file\n        for row in reader:\n            # Ensure the row has enough columns\n            if len(row) >= 2:\n                # Combine all 'value' values into a single line\n                quizz_values = ''.join(row[1:])  # Assuming 'value' starts at index 1\n                # Write the concatenated values to the output file\n                writer.writerow([quizz_values])\n            else:\n                print(f\"Skipping row {reader.line_num} due to insufficient columns.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T01:27:48.134247Z","iopub.execute_input":"2024-01-20T01:27:48.134582Z","iopub.status.idle":"2024-01-20T01:27:54.079765Z","shell.execute_reply.started":"2024-01-20T01:27:48.134557Z","shell.execute_reply":"2024-01-20T01:27:54.078912Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Replace 'input_file.csv' and 'output_file.csv' with your actual file names\ninput_file = '/kaggle/input/outputfile/Train Image submission.csv'\noutput_file = '/kaggle/working/Submission4.csv'\n\n# Read the CSV file\nwith open(input_file, 'r') as infile:\n    reader = csv.reader(infile)\n    header = next(reader)  # Read and skip the header\n\n    # Open the output CSV file for writing\n    with open(output_file, 'w', newline='') as outfile:\n        writer = csv.writer(outfile)\n\n        # Write the modified header to the output file\n        writer.writerow(['QUIZZ'] + [f'Digit_{i+1}' for i in range(81)])\n\n        # Process each row in the input file\n        for row in reader:\n            # Ensure the row has enough columns\n            if len(row) >= 82:  # Assuming 'value' starts at index 1\n                # Extract the 'value' values from the row\n                quizz_values = row[1:82]\n                # Write the extracted values to the output file\n                writer.writerow([''] + quizz_values)  # Adding an empty column for 'QUIZZ'\n            else:\n                print(f\"Skipping row {reader.line_num} due to insufficient columns.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T01:29:39.603803Z","iopub.execute_input":"2024-01-20T01:29:39.604456Z","iopub.status.idle":"2024-01-20T01:29:42.665726Z","shell.execute_reply.started":"2024-01-20T01:29:39.604423Z","shell.execute_reply":"2024-01-20T01:29:42.664705Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\nfile_path = '/kaggle/input/outputfile/Train Label submission.csv'  # Replace with your CSV file path\ndf = pd.read_csv(file_path)\n\n# Initialize an empty list to store the concatenated strings\nconcatenated_values = []\n\n# Iterate over the DataFrame in chunks of 81 rows\nfor start in range(0, len(df), 81):\n    end = start + 81\n    chunk = df.iloc[start:end]\n    concatenated_string = ''.join(map(str, chunk['value'].values))\n    concatenated_values.append(concatenated_string)\n\n# Create a new DataFrame with the concatenated strings\nnew_df = pd.DataFrame(concatenated_values, columns=['Concatenated Values'])\n\n# Save the new DataFrame to a CSV file\nnew_file_path = 'Train Label concatenated_values.csv'  # You can change the file name if needed\nnew_df.to_csv(new_file_path, index=False)\n\nprint(f\"File saved as {new_file_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T02:33:28.733167Z","iopub.execute_input":"2024-01-20T02:33:28.733838Z","iopub.status.idle":"2024-01-20T02:33:38.776473Z","shell.execute_reply.started":"2024-01-20T02:33:28.733805Z","shell.execute_reply":"2024-01-20T02:33:38.775573Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"File saved as Train Label concatenated_values.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\n\n# Replace 'file1.csv' and 'file2.csv' with your actual file names\nfile1 = '/kaggle/working/Train Image concatenated_values.csv'\nfile2 = '/kaggle/working/Train Label concatenated_values.csv'\noutput_file = 'trained_output_1.csv'\n\ncolumn_name_file1 = 'Quiz'\ncolumn_name_file2 = 'Solution'\n\n# Read data from the first CSV file\ndata1 = []\nwith open(file1, 'r') as csvfile1:\n    reader1 = csv.reader(csvfile1)\n    for row in reader1:\n        data1.append(row[0])  # Assuming the column to combine from file1 is the first column\n\n# Read data from the second CSV file\ndata2 = []\nwith open(file2, 'r') as csvfile2:\n    reader2 = csv.reader(csvfile2)\n    for row in reader2:\n        data2.append(row[0])  # Assuming the column to combine from file2 is the first column\n\n# Combine the two columns\ncombined_data = list(zip(data1, data2))\ncombined_header = [column_name_file1, column_name_file2]\n\n# Write the combined data to a new CSV file\nwith open(output_file, 'w', newline='') as csvfile_combined:\n    writer_combined = csv.writer(csvfile_combined)\n    writer_combined.writerow(combined_header)\n    # Write header if needed\n    # writer_combined.writerow(['Column_from_file1', 'Column_from_file2'])\n    \n    # Write the combined data\n    writer_combined.writerows(combined_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T03:09:13.722396Z","iopub.execute_input":"2024-01-20T03:09:13.722788Z","iopub.status.idle":"2024-01-20T03:09:14.204735Z","shell.execute_reply.started":"2024-01-20T03:09:13.722759Z","shell.execute_reply":"2024-01-20T03:09:14.203944Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/trained_final.csv')\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T03:14:02.780699Z","iopub.execute_input":"2024-01-20T03:14:02.781096Z","iopub.status.idle":"2024-01-20T03:14:02.898486Z","shell.execute_reply.started":"2024-01-20T03:14:02.781066Z","shell.execute_reply":"2024-01-20T03:14:02.897529Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                 Concatenated Values  \\\n0  0636554170018000590950008620700009000000700203...   \n1  1007540320002060070048390500010780000405903100...   \n2  7000508398000250409300782002798004000004970266...   \n3  0000930078030000290590824039003502016001007081...   \n4  6507300901986000070370002560003456020000010762...   \n\n                               Concatenated Values.1  \n0  6832954174218673597953418625721349968465771233...  \n1  1697548323582169472748396515314782967465923189...  \n2  7126548398659237419341782652798614533584971266...  \n3  2145936878637415297596824139783542616351297481...  \n4  6527348912986524374371982569713456823852619742...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Concatenated Values</th>\n      <th>Concatenated Values.1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0636554170018000590950008620700009000000700203...</td>\n      <td>6832954174218673597953418625721349968465771233...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1007540320002060070048390500010780000405903100...</td>\n      <td>1697548323582169472748396515314782967465923189...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7000508398000250409300782002798004000004970266...</td>\n      <td>7126548398659237419341782652798614533584971266...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0000930078030000290590824039003502016001007081...</td>\n      <td>2145936878637415297596824139783542616351297481...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6507300901986000070370002560003456020000010762...</td>\n      <td>6527348912986524374371982569713456823852619742...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import csv\n\n# Replace 'input_file.csv' and 'output_file.csv' with your actual file names\ninput_file = '/kaggle/working/trained_output_1.csv'\noutput_file = 'trained_final.csv'\n\n# Read the CSV file, skipping the first row\nwith open(input_file, 'r') as infile:\n    reader = csv.reader(infile)\n    header = next(reader)  # Read and skip the header\n\n    # Read the remaining rows\n    data = [row for row in reader]\n\n# Write the remaining rows to a new CSV file\nwith open(output_file, 'w', newline='') as outfile:\n    writer = csv.writer(outfile)\n\n    # Write the header to the output file if needed\n    # writer.writerow(header)\n\n    # Write the remaining rows to the output file\n    writer.writerows(data)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T03:13:45.144121Z","iopub.execute_input":"2024-01-20T03:13:45.144537Z","iopub.status.idle":"2024-01-20T03:13:45.976119Z","shell.execute_reply.started":"2024-01-20T03:13:45.144507Z","shell.execute_reply":"2024-01-20T03:13:45.975286Z"},"trusted":true},"execution_count":31,"outputs":[]}]}