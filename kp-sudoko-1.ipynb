{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7338196,"sourceType":"datasetVersion","datasetId":4260331}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# Load your data\ntrain_images = np.load('/kaggle/input/sudokotask1/train/imgs-001.npy')  # Shape: (50000, 252, 252)\ntrain_labels = np.load('/kaggle/input/sudokotask1/train/known_values_labels.npy')  # Shape: (50000, 41, 3)\ntest_images = np.load('/kaggle/input/sudokotask1/test/imgs.npy')    # Shape: (10000, 252, 252)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-04T15:58:01.077756Z","iopub.execute_input":"2024-01-04T15:58:01.078268Z","iopub.status.idle":"2024-01-04T15:58:11.967569Z","shell.execute_reply.started":"2024-01-04T15:58:01.078232Z","shell.execute_reply":"2024-01-04T15:58:11.966387Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nimport numpy as np\n\n# def preprocess_images_batch(images, batch_size):\n#     num_images = images.shape[0]\n#     cell_size = 28\n#     for batch_start in range(0, num_images, batch_size):\n#         batch_end = min(batch_start + batch_size, num_images)\n#         processed_batch = np.zeros(((batch_end - batch_start) * 81, cell_size, cell_size))\n\n#         for idx in range(batch_start, batch_end):\n#             for row in range(9):\n#                 for col in range(9):\n#                     cell = images[idx, row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size]\n#                     processed_cell = cell / 255.0\n#                     processed_batch[(idx - batch_start) * 81 + row * 9 + col] = processed_cell\n\n#         # Here you can save each batch to disk or yield it if using a generator\n#         yield processed_batch\n\n# # Example usage\n# batch_size = 100  # Adjust this based on your system's memory capacity\n# for processed_batch in preprocess_images_batch(train_images, batch_size):\n#     # Save or process each batch\n#     pass\n\n# for processed_batch in preprocess_images_batch(train_images, batch_size):\n#     # Save or process each batch\n#     pass\n# def preprocess_images_simple(images):\n#     num_images = images.shape[0]\n#     cell_size = 28\n#     processed_images = np.zeros((num_images * 81, cell_size, cell_size))\n\n#     for idx in range(num_images):\n#         for row in range(9):\n#             for col in range(9):\n#                 # Extracting each cell\n#                 cell = images[idx, row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size]\n#                 # Normalize the cell\n#                 processed_cell = cell / 255.0\n#                 # Store in processed_images\n#                 processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n#     return processed_images\ndef preprocess_images_simple(images, new_size=(14, 14)):\n    num_images = images.shape[0]\n    processed_images = np.zeros((num_images * 81, new_size[0], new_size[1]))\n\n    for idx in range(num_images):\n        for row in range(9):\n            for col in range(9):\n                # Extract each cell\n                cell = images[idx, row*28:(row+1)*28, col*28:(col+1)*28]\n                # Resize the cell\n                resized_cell = cv2.resize(cell, new_size, interpolation=cv2.INTER_AREA)\n                # Normalize the cell\n                processed_cell = resized_cell / 255.0\n                # Store in processed_images\n                processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n    return processed_images\n\n# Process the entire datasets with reduced resolution\ntrain_images_processed = preprocess_images_simple(train_images)\ntest_images_processed = preprocess_images_simple(test_images)\n\n# # Process the entire datasets\n# train_images_processed = preprocess_images_simple(train_images)\n# test_images_processed = preprocess_images_simple(test_images)\n\n\n# train_images_processed = preprocess_images_batch(train_images, batch_size)\n# test_images_processed = preprocess_images_batch(test_images, batch_size)\n# def preprocess_images(images):\n#     num_images = images.shape[0]\n#     # Each image is divided into 9x9 cells, each cell is approximately 28x28\n#     cell_size = 28\n#     processed_images = np.zeros((num_images * 81, cell_size, cell_size))\n\n#     for idx in range(num_images):\n#         for row in range(9):\n#             for col in range(9):\n#                 # Extracting each cell\n#                 cell = images[idx, row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size]\n#                 # Normalize the cell\n#                 processed_cell = cell / 255.0\n#                 # Store in processed_images\n#                 processed_images[idx * 81 + row * 9 + col] = processed_cell\n\n#     return processed_images\n\n# train_images_processed = preprocess_images(train_images)\n# test_images_processed = preprocess_images(test_images)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T15:58:11.970654Z","iopub.execute_input":"2024-01-04T15:58:11.971125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to convert train_labels to a format suitable for training\ndef prepare_labels(labels):\n    # Flatten the labels into a 1D array\n    # Each image contributes 81 (9x9) cells\n    num_images = len(labels)\n    new_labels = np.zeros((num_images * 81), dtype=int)\n\n    for i, label in enumerate(labels):\n        for cell in label:\n            x, y, value = cell\n            x, y, value = int(x), int(y), int(value)  # Ensure integers\n            if 0 <= x < 9 and 0 <= y < 9:  # Check for bounds\n                # Convert 2D coordinates to 1D index and set the value\n                new_labels[i * 81 + x * 9 + y] = value\n\n    return new_labels\n\ntrain_labels_processed = prepare_labels(train_labels)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# def build_model():\n#     model = Sequential([\n#         Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n#         MaxPooling2D(2, 2),\n\n#         Conv2D(64, (3, 3), activation='relu', padding='same'),\n#         MaxPooling2D(2, 2),\n\n#         Conv2D(128, (3, 3), activation='relu', padding='same'),\n#         MaxPooling2D(2, 2),\n\n#         Flatten(),\n#         Dense(128, activation='relu'),\n#         Dropout(0.5),\n#         Dense(10, activation='softmax')\n#     ])\n#     return model\ndef build_model():\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(14, 14, 1), padding='same'),\n        MaxPooling2D(2, 2),\n\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D(2, 2),\n\n        Flatten(),\n        Dense(64, activation='relu'),  # Reduced number of neurons\n        Dropout(0.5),\n        Dense(10, activation='softmax')  # 10 classes for digits 0-9\n    ])\n    return model\n\n# model = build_model()\n# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# model.summary()\n\nmodel = build_model()\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assuming train_images_processed and train_labels_processed are your full datasets\n# Split the first 25,000 for training and the rest for potential validation and testing\ntrain_images_subset = train_images_processed[:50000]\ntrain_labels_subset = train_labels_processed[:50000]\n\n# Further split the training data to create a validation set\n# Here, I'll use 20% of the 25,000 images for validation, but you can adjust this percentage\ntrain_images_final, val_images, train_labels_final, val_labels = train_test_split(\n    train_images_subset, train_labels_subset, test_size=0.2, random_state=42)\n\n# Reshape the datasets for the CNN\ntrain_images_final_reshaped = np.expand_dims(train_images_final, axis=-1)\nval_images_reshaped = np.expand_dims(val_images, axis=-1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmodel.fit(train_images_final_reshaped, train_labels_final, \n          validation_data=(val_images_reshaped, val_labels),\n          epochs=10, batch_size=8)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the test_images for prediction\ntest_images_reshaped = np.expand_dims(test_images_processed, axis=-1)\n\n# Predict\npredictions = model.predict(test_images_reshaped)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_file(predictions):\n    with open('submission2.csv', 'w') as f:\n        f.write('id,value\\n')\n        for img_id in range(len(predictions)):\n            for y in range(9):\n                for x in range(9):\n                    cell_id = f\"{img_id}_{x}{y}\"\n                    value = int(predictions[img_id][x * 9 + y])\n                    f.write(f\"{cell_id},{value}\\n\")\n\ncreate_submission_file(predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission_file(predictions, num_images):\n    with open('submission.csv', 'w') as f:\n        f.write('id,value\\n')\n        for img_id in range(num_images):\n            for position in range(81):  # Iterate over each cell in the grid\n                x = position // 9  # x-coordinate (row)\n                y = position % 9   # y-coordinate (column)\n                cell_id = f\"{img_id}_{x}{y}\"\n                # Calculate the index in the predictions array\n                pred_index = img_id * 81 + position\n                # Get the most likely digit for this cell\n                value = np.argmax(predictions[pred_index])\n                f.write(f\"{cell_id},{value}\\n\")\n\ncreate_submission_file(predictions, num_images=10000)  # Replace 50000 with your actual number of images\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictions.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}